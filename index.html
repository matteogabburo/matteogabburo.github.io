<!DOCTYPE html>
<html>
<head>
    <title>Matteo Gabburo</title>

    <!-- My css -->
    <style>

        body{
            background-color: whitesmoke;
        }

        .mainblock {
            background-color: rgb(228, 218, 211);
            margin: 10px;
            padding: 10px;
            border-style:ridge;
        }

        #title {
            background-color: whitesmoke;
            border-style:none;
        }

        #title h3 {
            margin-top: -30px;
        }

        .article .title {
            font-size: large;
            font-weight: bold;
        }
    </style>

</head>

<body>

<div class="mainblock" id="title">
    <h1>
        Matteo Gabburo
    </h1>
    <h3>
        ~ PhD Student @ University of Trento
    </h3>
</div>

<div class="mainblock" id="aboutme">
    <h2>
        About me:
    </h2>
    <p>
        Hi, I'm <strong>Matteo</strong>! I'm a PhD student in Information and Communication Technology at the University of Trento.
    </p>
    <p>
        My research interests involve Natural Language Processing, Question Answering and in specific Generative models for Question Answering. Currently I'm focusing my interest in new automatic metrics to evaluate Generative QA models.
    </p>
</div>

<div class="mainblock" id="education">
    <h2>
        Education:
    </h2>
    <p>
        <div class="education-list">
        <ul>
            <li class="phd">
                <span class="period"> November 2020 - Present</span><br>
                <span class="degree"> &#8594; <strong>PhD student </strong><it>@ University of Trento (Italy)</it></span><br>
            </li>
            <li class="msc">
                <span class="period"> September 2016 - March 2019</span><br>
                <span class="degree"> &#8594; <strong>MSc student </strong><it></it>@ University of Trento (Italy)</it></span><br>
            </li>
            <li class="bsc">
                <span class="period"> September 2012 - March 2016</span><br>
                <span class="degree"> &#8594; <strong>BSc student </strong><it></it>@ University of Trento (Italy)</it></span><br>
            </li>
            <li class="ssd">
                <span class="period"> September 2007 - June 2012</span><br>
                <span class="degree"> &#8594; <strong>Secondary School Diploma in IT </strong><it></it>@ ITIS Fermi of Bassano del Grappa (Italy)</it></span><br>
            </li>
        </ul>
        </div>
    </p>
</div>


<div class="mainblock" id="experience">
    <h2>
        Experience:
    </h2>    
    <div class="experiences-list">
        <ul>
            <li class="jobposition">
                <span class="period"> February 2022 - Present</span><br>
                <span class="jobtitle"> &#8594; <strong>Applied Scientist Intern @ Amazon Alexa </strong><it> Los Angeles (USA)</it></span><br>
            </li>
            <li class="jobposition">
                <span class="period"> April 2021 - October 2021</span><br>
                <span class="jobtitle"> &#8594; <strong>Applied Scientist Intern @ Amazon Alexa </strong><it> Los Angeles (USA)</it></span><br>
            </li>
            <li class="jobposition">
                <span class="period"> July 2019 - July 2020</span><br>
                <span class="jobtitle"> &#8594; <strong>Borsa di ricerca post laurea </strong><it></it>@ University of Trento (Italy)</it></span><br>
            </li>
        </ul>
        </div>
    </p>
</div>


<div class="mainblock" id="publications">
    <h2>
        Publications:
    </h2>    
    <ul>
        <li class="article">
            <span class="title"> Effective Pre-Training Objectives for Transformer-based Autoencoders </span><br>
            <span class="authors"> Luca Di Liello, Matteo Gabburo, Alessandro Moschitti </span><br>
            <span class="venue">  &#8594; <strong>EMNLP22, findings, SustainNLP Workshop</strong> </span><br>
            <span class="resource"> <a href="https://arxiv.org/pdf/2210.13536.pdf">PDF</a> </span><br>
            <p class="abstract">
            In this paper, we study trade-offs between efficiency, cost and accuracy when pre-training Transformer encoders with different pre-training objectives. For this purpose, we analyze features of common objectives and combine them to create new effective pre-training approaches. Specifically, we designed light token generators based on a straightforward statistical approach, which can replace ELECTRA computationally heavy generators, thus highly reducing cost. Our experiments also show that (i) there are more efficient alternatives to BERT's MLM, and (ii) it is possible to efficiently pre-train Transformer-based models using lighter generators without a significant drop in performance. 
            </p>
        </li>

        <li class="article">
            <span class="title"> Knowledge Transfer from Answer Ranking to Answer Generation </span><br>
            <span class="authors"> Matteo Gabburo, Rik Koncel-Kedziorski, Siddhant Garg, Luca Soldaini, Alessandro Moschitti </span><br>
            <span class="venue">  &#8594; <strong>EMNLP22, main conference</strong> </span><br>
            <span class="resource"> <a href="https://preview.aclanthology.org/emnlp-22-ingestion/2022.emnlp-main.645.pdf">PDF</a> </span><br>
            <p class="abstract">
                Recent studies show that Question Answering (QA) based on Answer Sentence Selection (AS2) can be improved by generating an improved answer from the top-k ranked answer sentences (termed GenQA). This allows for synthesizing the information from multiple candidates into a concise, natural-sounding answer. However, creating large-scale supervised training data for GenQA models is very challenging. In this paper, we propose to train a GenQA model by transferring knowledge from a trained AS2 model, to overcome the aforementioned issue. First, we use an AS2 model to produce a ranking over answer candidates for a set of questions. Then, we use the top ranked candidate as the generation target, and the next k top ranked candidates as context for training a GenQA model. We also propose to use the AS2 model prediction scores for loss weighting and score-conditioned input/output shaping, to aid the knowledge transfer. Our evaluation on three public and one large industrial datasets demonstrates the superiority of our approach over the AS2 baseline, and GenQA trained using supervised data.    
            </p>
        </li>
    </ul>
</div>

</body>

</html>